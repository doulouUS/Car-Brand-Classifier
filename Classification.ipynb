{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707a73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ee00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path to data\n",
    "data_root = Path(\"../tfrecords/car-brand/\")\n",
    "train_data_path = data_root / \"train.record\"\n",
    "val_data_path = data_root / \"test.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c569f0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting source_dir/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source_dir/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Installing tf add-ons to obtain Confusion Matrix in the callback metrics \n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "install('tensorflow-addons')\n",
    "print(\"Installed tf add-ons\")\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(\"Device:\", tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "CLASSES = [\n",
    "    # 'notvisible', filtered out\n",
    "    'dacia',\n",
    "    'renault',\n",
    "    'peugeot',\n",
    "    'fiat',\n",
    "    'hyundai',\n",
    "    'volkswagen',\n",
    "    'citroen',\n",
    "    'ford',\n",
    "    'other' # encompass all other brands that have less than 50 training samples\n",
    "]\n",
    "\n",
    "class ConfusionMatrixCallBack(keras.callbacks.Callback):\n",
    "    def __init__(self, class_id2label):\n",
    "        super(ConfusionMatrixCallBack, self).__init__()\n",
    "        self.class_id2label = class_id2label\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Epoch {} of training\".format(epoch))\n",
    "        print(f\"##### Confusion matrix ########\")\n",
    "        if logs:\n",
    "            for class_id, conf_matrix in enumerate(logs['multiConfusion']):\n",
    "                print(f\"Class {class_id} - {self.class_id2label[class_id]}:\")\n",
    "                print(f\"{conf_matrix}\")\n",
    "\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(f\"Available log keys: {keys}\")\n",
    "        print(f\"##### Confusion matrix ########\")\n",
    "        for class_id, conf_matrix in enumerate(logs['multiConfusion']):\n",
    "            print(f\"Class {class_id} - {self.class_id2label[class_id]}:\")\n",
    "            print(f\"{conf_matrix}\")\n",
    "            \n",
    "        \n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/brand': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "    )\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image/encoded'])\n",
    "    label = tf.cast(example['image/class/brand'], tf.string)\n",
    "    return image, label\n",
    "\n",
    "def pre_process(image, label):\n",
    "    \n",
    "    # label pre-processing: string -> one-hot vector\n",
    "    output_label = string2classid.lookup(label)\n",
    "    output_label = tf.one_hot(\n",
    "      indices=output_label, depth=len(CLASSES), on_value=1.0, off_value=0.0)\n",
    "    return image, output_label   \n",
    "\n",
    "def load_dataset(filenames):\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames\n",
    "    )  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(\n",
    "        ignore_order\n",
    "    )  # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(\n",
    "        read_tfrecord, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def filter_dataset(image, label):\n",
    "    # Remove samples with label equals to 'notvisible'\n",
    "    return label != \"notvisible\"\n",
    "\n",
    "def get_dataset(filenames, shuffle=True):\n",
    "    dataset = load_dataset(filenames)\n",
    "    \n",
    "    # Filter\n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "    \n",
    "    # Preprocessing\n",
    "    dataset = dataset.map(\n",
    "      pre_process, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "def make_overfit_model():\n",
    "    \"\"\"Creates a model without regularization, data augmentation.\n",
    "    Used to test if learning is happening on a base model before iterating.\n",
    "\n",
    "    \"\"\"\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input([*IMAGE_SIZE, 3])\n",
    "    \n",
    "    # Preprocess_input steps:\n",
    "    # tf32\n",
    "    # cast to -1, 1\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "\n",
    "    x = base_model(x)\n",
    "        \n",
    "    # -------------- New head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(len(CLASSES), activation=\"softmax\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(\n",
    "                name='acc', dtype=None\n",
    "            ),\n",
    "            tfa.metrics.MultiLabelConfusionMatrix(\n",
    "                num_classes=len(CLASSES),\n",
    "                name='multiConfusion', dtype=None\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_model():\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "    # ------------ data augmentation\n",
    "    data_augmentation = keras.Sequential(\n",
    "       [\n",
    "           tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "           tf.keras.layers.experimental.preprocessing.RandomRotation(0.25),\n",
    "           tf.keras.layers.experimental.preprocessing.RandomContrast(.2)\n",
    "       ]\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input([*IMAGE_SIZE, 3])\n",
    "    \n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # Preprocessing here already includes normalization between -1 and 1\n",
    "    x = tf.keras.applications.xception.preprocess_input(x)\n",
    "    x = base_model(x)\n",
    "    \n",
    "    # TODO\n",
    "    # 1. overfit [OK]\n",
    "    #  run on only one batch (overfit keras option) + obtain the confusion matrix \n",
    "    #   freeze and unfreeze\n",
    "    \n",
    "    # 2. Remove regularization [ONGOING]\n",
    "    #   - dropout\n",
    "    #   - data augmentation\n",
    "    # 3. Regularize\n",
    "    \n",
    "    # 4. Error Analysis\n",
    "    #  examine 100 images with predictions\n",
    "        \n",
    "    # -------------- New head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.7)(x)\n",
    "    outputs = tf.keras.layers.Dense(len(CLASSES), activation=\"softmax\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(\n",
    "                name='acc', dtype=None\n",
    "            ),\n",
    "            tfa.metrics.MultiLabelConfusionMatrix(\n",
    "                num_classes=len(CLASSES),\n",
    "                name='multiConfusion', dtype=None\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    #============= CLI Commands =================\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch_size', type=int, default=5)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('--image_size', type=int, default=600)\n",
    "    parser.add_argument('--overfit', type=bool, default=False)\n",
    "    parser.add_argument('--small_data', type=bool, default=False)\n",
    "\n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    model_artefacts_output_path = os.environ['SM_MODEL_DIR']\n",
    "    other_artefacts_output_path = os.environ['SM_OUTPUT_DATA_DIR']\n",
    "    \n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\", args.train)\n",
    "    \n",
    "    #============= Hyperparams =================\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    IMAGE_SIZE = [int(args.image_size), int(args.image_size)]\n",
    "\n",
    "    label_to_class_id = {\n",
    "        label: class_id for class_id, label in enumerate(sorted(CLASSES))\n",
    "    }\n",
    "\n",
    "    print(\"Writing mapping label to class id\")\n",
    "    with open(os.path.join(other_artefacts_output_path, \"label_2_class_id.txt\"), \"w\") as f:\n",
    "        json.dump(label_to_class_id, f)\n",
    "\n",
    "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=CLASSES,\n",
    "        values=tf.cast(tf.range(len(CLASSES)), tf.int32),\n",
    "        key_dtype=tf.string,\n",
    "        value_dtype=tf.int32\n",
    "    )\n",
    "    # Remark: if key is not found (class not considered in the classifcation task), \n",
    "    # assign the label len(CLASSES)-1 corresponding to 'other' class.\n",
    "    string2classid = tf.lookup.StaticHashTable(initializer, default_value=len(CLASSES)-1)\n",
    "\n",
    "    #============= Datasets =================\n",
    "    if not args.small_data:\n",
    "        train_dataset = get_dataset(\n",
    "            [str(i) for i in Path(args.train).glob(\"*.record\")]\n",
    "        )\n",
    "    \n",
    "        val_dataset = get_dataset(\n",
    "            [str(i) for i in Path(args.test).glob(\"*.record\")],\n",
    "            shuffle=False\n",
    "        )\n",
    "    else:\n",
    "        train_dataset = get_dataset(\n",
    "            [str(i) for i in Path(args.train).glob(\"*.record\")]\n",
    "        ).take(10)\n",
    "    \n",
    "        val_dataset = get_dataset(\n",
    "            [str(i) for i in Path(args.test).glob(\"*.record\")],\n",
    "            shuffle=False\n",
    "        ).take(5)\n",
    "\n",
    "    # TODO: does not work, returns -2\n",
    "    print(f\"Number of training samples: {tf.data.experimental.cardinality(train_dataset)}\")\n",
    "    print(f\"Number of test samples    : {tf.data.experimental.cardinality(val_dataset)}\")\n",
    "\n",
    "    #============= Learning rate =================\n",
    "    initial_learning_rate = args.learning_rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=200, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    #============= Callbacks =================\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"classifier_car_brand_{epoch}.h5\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    confusion_matrix_cb = ConfusionMatrixCallBack(\n",
    "        class_id2label={v: k for k, v in label_to_class_id.items()}\n",
    "    )\n",
    "    \n",
    "    # Create folder containing tensorboard logs, under the path that SageMaker \n",
    "    # collects and save to S3 at the end of the training\n",
    "    Path(os.path.join(other_artefacts_output_path,\"./logs\")).mkdir(parents=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(other_artefacts_output_path,\"./logs\")\n",
    "    )\n",
    "\n",
    "    #============= Model =================\n",
    "    with strategy.scope():\n",
    "        if args.overfit:\n",
    "            model = make_overfit_model()\n",
    "        else:\n",
    "            model = make_model()\n",
    " \n",
    "    print(model.summary())\n",
    "\n",
    "    #============= Fitting =================\n",
    "    print(\"Starting Training\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        # class_weight=WEIGHTS_RATIO,\n",
    "        epochs=args.epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb, confusion_matrix_cb, tensorboard_callback],\n",
    "        verbose=2\n",
    "    )\n",
    "    print(\"Saving history\")\n",
    "    with open(os.path.join(other_artefacts_output_path, 'train_history_dict.pkl'), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    print(\"Saving model\")\n",
    "    model.save(os.path.join(model_artefacts_output_path, \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57537b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore visualizations to investigate the data more effectively\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.show_examples(dataset, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379b825",
   "metadata": {},
   "source": [
    "## Tensorflow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "432b6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment = 2\n",
    "\n",
    "s3_root = f\"s3://axa-iclaim-preprod-bucket/AXA-GETD-CAR_DAMAGE-DATA/tensorflow-keras-classification-experiments/{experiment}\"\n",
    "s3_train_data = os.path.join(s3_root, \"data\", \"train.record\")\n",
    "s3_test_data = os.path.join(s3_root, \"data\", \"test.record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data locally is not present in S3\n",
    "!aws s3 cp {str(train_data_path)} {s3_train_data}\n",
    "!aws s3 cp {str(val_data_path)} {s3_test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f910df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "hyperparam = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 5,\n",
    "    'learning_rate': .01,\n",
    "    'image_size': 600,  # size in pixels to resize images in the pipeline to (image_size, image_size)\n",
    "    'overfit': True, # use a base model without regularization, data augmentation\n",
    "    'small_data': True  # use only 10 images from train set, 5 from test set\n",
    "}\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    model_dir=s3_root,\n",
    "    use_spot_instances=True,\n",
    "    max_wait=86401,\n",
    "    hyperparameters=hyperparam,\n",
    "    entry_point=\"source_dir/train.py\",\n",
    "    role=role,\n",
    "    volume_size=20,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.4xlarge',  #\"ml.g4dn.4xlarge\",\n",
    "    framework_version=\"2.4.1\",  # do not change\n",
    "    py_version=\"py37\",\n",
    "    base_job_name='keras-classifier-car-brand-overfit'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c3e0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit(\n",
    "    {\n",
    "        \"train\": s3_train_data,  #f\"file://{str(train_data_path.resolve())}\", # s3_train_data,  # \n",
    "        \"test\" : s3_test_data  #f\"file://{str(val_data_path.resolve())}\" # s3_test_data,  #\n",
    "    },\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95127978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(5, 5, n + 1)\n",
    "        plt.imshow(image_batch[n] / 255.0)\n",
    "        plt.title(\"Image\")\n",
    "\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}